{"cells":[{"metadata":{"trusted":true,"_uuid":"da0236e4b36ce514c1fec3fd72f236d1fa259131"},"cell_type":"code","source":"def import_libs():\n    # Importing the libraries\n    global np; import numpy as np\n    global plt; import matplotlib.pyplot as plt; plt.style.use('fivethirtyeight')\n    global pd; import pandas as pd\n    global MinMaxScaler; from sklearn.preprocessing import MinMaxScaler\n    global Sequential; from keras.models import Sequential\n    global Dense, LSTM, Dropout, GRU, Bidirectional; from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n    global SGD; from keras.optimizers import SGD\n    global math; import math\n    global mean_squared_error; from sklearn.metrics import mean_squared_error\nimport_libs()\n# rcParams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b288a8e2caf6196daec9cd2bc4ca78fe50345845"},"cell_type":"code","source":"# Some functions to help out with\ndef plot_predictions(test,predicted):\n    plt.plot(test, color='red',label='Real IBM Stock Price')\n    plt.plot(predicted, color='blue',label='Predicted IBM Stock Price')\n    plt.title('IBM Stock Price Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('IBM Stock Price')\n    plt.legend()\n    plt.show()\n\ndef return_rmse(test,predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {}.\".format(rmse))\n\n\n# First, we get the data\ndataset = pd.read_csv('../input/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\nprint('Tail of the dataset is: \\n\\n {}:'.format(dataset.tail()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf5a9463d58e73852d2b70be9611e8cf1f4166fd"},"cell_type":"code","source":"# Checking for missing values\ntraining_set = dataset[:'2016'].iloc[:,1:2].values\ntest_set = dataset['2017':].iloc[:,1:2].values\n# dataset[:'2016'].iloc[:,0:2].isnull().sum()\n# len(dataset[:'2016'].iloc[:,1:2].values)\n\n# We have chosen 'High' attribute for prices. Let's see what it looks like\ndataset[\"High\"][:'2016'].plot(figsize=(16,4),legend=True)\ndataset[\"High\"]['2017':].plot(figsize=(16,4),legend=True)\nplt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\nplt.title('IBM stock price');plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fccfb866a2b4c702e0b2742f7c0289512d713d1b"},"cell_type":"code","source":"# Scaling the training set\nsc = MinMaxScaler(feature_range=(0,1)); training_set_scaled = sc.fit_transform(training_set)\n# print(training_set_scaled[:20], end='\\n\\n'); print(training_set[:20])\n\n\ndef X_y_split(lookback):\n    # Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n    # So for each element of training set, we have 60 previous training set elements \n    X_train=[training_set_scaled[i-lookback:i,0] for i in range(lookback, len(training_set))]\n    y_train=[training_set_scaled[i,0] for i in range(lookback, len(training_set))]\n    # X_train = []; y_train = []\n    # for i in range(60,len(training_set)): # len(training_set) # = 2769\n    #     X_train.append(training_set_scaled[i-60:i,0])\n    #     y_train.append(training_set_scaled[i,0])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    # X_train[:20][1]; X_train.shape; y_train.shape\n    return X_train, y_train\n\nX_train, y_train=X_y_split(60)\n\n\ndef X_y_reshape(single_y_size, x, y):\n    if float(single_y_size).is_integer():\n        x=[x[i] for i in range(0,len(y)-len(y)%single_y_size,single_y_size)]\n        y=[y[i:i+single_y_size] for i in range(0,len(y)-len(y)%single_y_size,single_y_size)]\n    else:\n        print('Need an integer!')\n    return np.array(x), np.array(y)\n\nX_train, y_train=X_y_reshape(1,X_train, y_train)\n\n# Reshaping X_train for efficient modelling, why do we need another index?? \n# .fit() in the next codeblock somehow expects 3 dimensions for the X_input \nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1)) # 2709 x 60 x 1, 1 means only one feature 'price'\n# Always give a 3D array as an input to LSTM network. \n# 1st dimension represents the number of samples (or batch size)\n# 2nd dimension represents the number of time-steps you are feeding a sequence. \n# 3rd dimension represents the number of units/features in one input sequence.\n# X_train.shape  # X_train[:][:][0]\nprint(X_train.shape);print(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Num_params_1st_layer= 4xUNITSx(units+1+1)=4x50x(50+1+1)=10400; units is the number of outputs of this layer\n* Num_params_2nd_layer= 4xUNITSx(units+output_from_1st_layer+1)=4x50x(50+1+1)=10400"},{"metadata":{"trusted":true,"_uuid":"df20eb7e8062dae0a3aff2182aa440faddd0017d"},"cell_type":"code","source":"def build(X, y, batch_size, epoch):\n    # The LSTM architecture\n    regressor = Sequential()\n    #=======================================================================================================================\n    # return_sequences: Whether to return the last output. in the output sequence, or the full sequence. Default: False.\n    regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1],1))) \n    #if using batch_input_shape=( , , ), the 1st number 'batch_size' must be a factor of 2709\n    # we cannot really give any batch_size here if using batch_input_shape= (, ,), but if we use input_shape=(), we can later\n    # specify the batch_size in fit(), which doesn't reshape the actual input data.\n    regressor.add(Dropout(0.2))\n    #=======================================================================================================================\n    regressor.add(LSTM(units=60, return_sequences=True));regressor.add(Dropout(0.2))\n    # 1st dimension of output is None because we do not know the batch size in advance. See the console log below\n    #=======================================================================================================================\n    regressor.add(LSTM(units=50, return_sequences=True));regressor.add(Dropout(0.2))\n    #=======================================================================================================================\n    regressor.add(LSTM(units=50));regressor.add(Dropout(0.2))\n    # The output layer\n    regressor.add(Dense(units=1))\n\n    # Compiling the RNN\n    regressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n    regressor.summary()\n    history=regressor.fit(X,y,epochs=epoch, batch_size=batch_size)\n    return regressor\n\nregressor= build(X_train, y_train, batch_size=50, epoch=30)\n\n# # Fitting to the training set\n# history=regressor.fit(X_train,y_train,epochs=30,batch_size=32) # history.history.keys() shows dict_keys(['loss'])\n# # vars(history) shows what 'history' class has to offer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"326fa85615622feb484cc4c848edeec6f7133913"},"cell_type":"code","source":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so the first 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"High\"][:'2016'],dataset[\"High\"]['2017':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values # 251+60 =311\n# print(len(dataset_total[len(dataset_total)-len(test_set) - 60:].values)) # =311\n# len(test_set) # = 251\n# len(dataset_total) # = 3020\n\n#shape into one column as indicated by '1', but the row number has to be compatible with the original list\ninputs = inputs.reshape(-1,1); #(-1, ) simply means that it is an unknown dimension and we want numpy to figure it out.\n# print(inputs) #gives ONE column of numbers\ninputs  = sc.transform(inputs)\n# print(inputs.shape) # (311, 1)\n# print(len(test_set)) # =251","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"435b8024814939ac4fbd372baa0cd8cfc78f80bc"},"cell_type":"code","source":"def test_prep(lookback,data):\n    # Preparing X_test and predicting the prices\n    X_test = [data[i-lookback:i,0] for i in range(lookback,lookback+len(test_set))]\n    X_test = np.array(X_test); X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n#     print(X_test.shape) # (251, 60, 1)\n    return X_test\n\nX_test=test_prep(60,inputs) # (251, 60, 1)\n\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)\n\n# Visualizing the results for LSTM\nplot_predictions(test_set,predicted_stock_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6f6db0b6e1f17ac63c06ce49856873d98ba5f00"},"cell_type":"code","source":"# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(LSTM.units)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}